@article{becker_templates_2011,
	title = {Templates for convex cone problems with applications to sparse signal recovery},
	volume = {3},
	issn = {1867-2949, 1867-2957},
	url = {http://link.springer.com/10.1007/s12532-011-0029-5},
	doi = {10.1007/s12532-011-0029-5},
	abstract = {This paper develops a general framework for solving a variety of convex cone problems that frequently arise in signal processing, machine learning, statistics, and other ?elds. The approach works as follows: ?rst, determine a conic formulation of the problem; second, determine its dual; third, apply smoothing; and fourth, solve using an optimal ?rst-order method. A merit of this approach is its ?exibility: for example, all compressed sensing problems can be solved via this approach. These include models with objective functionals such as the total-variation norm, W x 1 where W is arbitrary, or a combination thereof. In addition, the paper also introduces a number of technical contributions such as a novel continuation scheme, a novel approach for controlling the step size, and some new results showing that the smooth and unsmoothed problems are sometimes formally equivalent. Combined with our framework, these lead to novel, stable and computationally e?cient algorithms. For instance, our general implementation is competitive with state-of-the-art methods for solving intensively studied problems such as the {LASSO}. Further, numerical experiments show that one can solve the Dantzig selector problem, for which no e?cient large-scale solvers exist, in a few hundred iterations. Finally, the paper is accompanied with a software release. This software is not a single, monolithic solver; rather, it is a suite of programs and routines designed to serve as building blocks for constructing complete algorithms.},
	pages = {165--218},
	number = {3},
	journaltitle = {Mathematical Programming Computation},
	author = {Becker, Stephen R. and Candes, Emmanuel J. and Grant, Michael C.},
	urldate = {2018-11-20},
	date = {2011-09},
	langid = {english},
}

@misc{pavan_ramkumar_2017_291992,
  author       = {Pavan Ramkumar and
                  Mainak Jas and
                  Titipat Achakulvisut and
                  Aid Idrizovic and
                  themantalope and
                  Daniel E. Acuna and
                  Vinicius Marques and
                  Hugo L Fernandes and
                  Eva Dyer},
  title        = {glm-tools/pyglmnet: First stable release},
  month        = feb,
  year         = 2017,
  doi          = {10.5281/zenodo.291992},
  url          = {https://doi.org/10.5281/zenodo.291992}
}

                  
@software{noauthor_structured_2018,
	title = {Structured Machine Learning in Python. Contribute to neurospin/pylearn-parsimony development by creating an account on {GitHub}},
	rights = {View license},
	url = {https://github.com/neurospin/pylearn-parsimony},
	publisher = {{NeuroSpin}},
	urldate = {2018-11-20},
	date = {2018-10-08},
	note = {original-date: 2014-07-04T14:11:47Z}
}


@software{breheny_regularization_2018,
	title = {Regularization paths for regression models with grouped covariates: pbreheny/grpreg},
	url = {https://github.com/pbreheny/grpreg},
	shorttitle = {Regularization paths for regression models with grouped covariates},
	author = {Breheny, Patrick},
	urldate = {2018-11-20},
	date = {2018-09-28},
	note = {original-date: 2012-10-09T13:06:22Z}
}


@software{greenfeld_cookiecutter_2018,
	title = {Cookiecutter template for a Python package. Contribute to audreyr/cookiecutter-pypackage development by creating an account on {GitHub}},
	rights = {View license},
	url = {https://github.com/audreyr/cookiecutter-pypackage},
	author = {Greenfeld, Audrey Roy},
	urldate = {2018-11-20},
	date = {2018-11-19},
	note = {original-date: 2013-07-14T18:52:05Z}
}


@online{noauthor_pypi_nodate,
	title = {{PyPI}  the Python Package Index},
	url = {https://pypi.org/},
	abstract = {The Python Package Index ({PyPI}) is a repository of software for the Python programming language.},
	titleaddon = {{PyPI}},
	urldate = {2018-11-20},
	langid = {english}
}


@software{noauthor_template_2018,
	title = {A template for scikit-learn extensions. Contribute to scikit-learn-contrib/project-template development by creating an account on {GitHub}},
	rights = {{BSD}-3-Clause},
	url = {https://github.com/scikit-learn-contrib/project-template},
	publisher = {scikit-learn-contrib},
	urldate = {2018-11-20},
	date = {2018-11-18},
	note = {original-date: 2016-01-15T20:54:59Z}
}


@article{tibshirani_regression_1996,
	title = {Regression Shrinkage and Selection via the Lasso},
	volume = {58},
	issn = {0035-9246},
	url = {http://www.jstor.org/stable/2346178},
	abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	pages = {267--288},
	number = {1},
	journaltitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Tibshirani, Robert},
	date = {1996},
	file = {2346178.pdf:C\:\\Users\\danke\\Zotero\\storage\\8ABCEWVZ\\2346178.pdf:application/pdf}
}


@article{boyd_distributed_2011,
	title = {Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers},
	volume = {3},
	issn = {1935-8237, 1935-8245},
	url = {http://www.nowpublishers.com/article/Details/MAL-016},
	doi = {10.1561/2200000016},
	abstract = {Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers},
	pages = {1--122},
	number = {1},
	journaltitle = {Foundations and Trends® in Machine Learning},
	shortjournal = {{MAL}},
	author = {Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan},
	urldate = {2017-07-26},
	date = {2011-07-26},
	file = {admm_distr_stats.pdf:C\:\\Users\\danke\\Zotero\\storage\\CQQSSWWH\\admm_distr_stats.pdf:application/pdf}
}



@incollection{mairal_network_2010,
	title = {Network Flow Algorithms for Structured Sparsity},
	url = {http://papers.nips.cc/paper/3965-network-flow-algorithms-for-structured-sparsity.pdf},
	pages = {1558--1566},
	booktitle = {Advances in Neural Information Processing Systems 23},
	publisher = {Curran Associates, Inc.},
	author = {Mairal, Julien and Jenatton, Rodolphe and Bach, Francis R. and Obozinski, Guillaume R},
	editor = {Lafferty, J. D. and Williams, C. K. I. and Shawe-Taylor, J. and Zemel, R. S. and Culotta, A.},
	urldate = {2018-12-04},
	date = {2010},
	file = {NIPS Full Text PDF:C\:\\Users\\danke\\Zotero\\storage\\6K6A8BCW\\Mairal et al. - 2010 - Network Flow Algorithms for Structured Sparsity.pdf:application/pdf}
}


@article{zhao_composite_2009,
	title = {The composite absolute penalties family for grouped and hierarchical variable selection},
	volume = {37},
	issn = {0090-5364, 2168-8966},
	url = {http://projecteuclid.org/euclid.aos/1250515393},
	doi = {10.1214/07-AOS584},
	abstract = {Extracting useful information from high-dimensional data is an important focus of todays statistical research and practice. Penalized loss function minimization has been shown to be effective for this task both theoretically and empirically. With the virtues of both regularization and sparsity, the L1-penalized squared error minimization method Lasso has been popular in regression models and beyond. In this paper, we combine different norms including L1 to form an intelligent penalty in order to add side information to the fitting of a regression or classification model to obtain reasonable estimates. Specifically, we introduce the Composite Absolute Penalties ({CAP}) family, which allows given grouping and hierarchical relationships between the predictors to be expressed. {CAP} penalties are built by defining groups and combining the properties of norm penalties at the across-group and within-group levels. Grouped selection occurs for nonoverlapping groups. Hierarchical variable selection is reached by defining groups with particular overlapping patterns. We propose using the {BLASSO} and cross-validation to compute {CAP} estimates in general. For a subfamily of {CAP} estimates involving only the L1 and L8 norms, we introduce the {iCAP} algorithm to trace the entire regularization path for the grouped selection problem. Within this subfamily, unbiased estimates of the degrees of freedom (df) are derived so that the regularization parameter is selected without cross-validation. {CAP} is shown to improve on the predictive performance of the {LASSO} in a series of simulated experiments, including cases with p»n and possibly mis-specified groupings. When the complexity of a model is properly calculated, {iCAP} is seen to be parsimonious in the experiments.},
	pages = {3468--3497},
	number = {6},
	journaltitle = {The Annals of Statistics},
	shortjournal = {Ann. Statist.},
	author = {Zhao, Peng and Rocha, Guilherme and Yu, Bin},
	urldate = {2018-12-04},
	date = {2009-12},
	mrnumber = {MR2549566},
	zmnumber = {1369.62164},
	keywords = {coefficient paths, grouped selection, hierarchical models, Linear regression, penalized regression, variable selection},
	file = {Full Text PDF:C\:\\Users\\danke\\Zotero\\storage\\WVK3QLIZ\\Zhao et al. - 2009 - The composite absolute penalties family for groupe.pdf:application/pdf}
}


@article{obozinski_group_2011,
	title = {Group Lasso with Overlaps: the Latent Group Lasso approach},
	url = {http://arxiv.org/abs/1110.0413},
	shorttitle = {Group Lasso with Overlaps},
	abstract = {We study a norm for structured sparsity which leads to sparse linear predictors whose supports are unions of prede ned overlapping groups of variables. We call the obtained formulation latent group Lasso, since it is based on applying the usual group Lasso penalty on a set of latent variables. A detailed analysis of the norm and its properties is presented and we characterize conditions under which the set of groups associated with latent variables are correctly identi ed. We motivate and discuss the delicate choice of weights associated to each group, and illustrate this approach on simulated data and on the problem of breast cancer prognosis from gene expression data.},
	journaltitle = {{arXiv}:1110.0413 [cs, stat]},
	author = {Obozinski, Guillaume and Jacob, Laurent and Vert, Jean-Philippe},
	urldate = {2018-10-08},
	date = {2011-10-03},
	eprinttype = {arxiv},
	eprint = {1110.0413},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv\:1110.0413 PDF:C\:\\Users\\danke\\Zotero\\storage\\HBYZ6PIU\\Obozinski et al. - 2011 - Group Lasso with Overlaps the Latent Group Lasso .pdf:application/pdf}
}

@article{yuan_model_2006,
	title = {Model selection and estimation in regression with grouped variables},
	volume = {68},
	issn = {1467-9868},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2005.00532.x/abstract},
	doi = {10.1111/j.1467-9868.2005.00532.x},
	abstract = {Summary.  We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multifactor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the {LARS} algorithm and the non-negative garrotte for factor selection. The lasso, the {LARS} algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.},
	pages = {49--67},
	number = {1},
	journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	shortjournal = {JRSSB},	
	author = {Yuan, Ming and Lin, Yi},
	urldate = {2015-02-05},
	date = {2006},
	langid = {english},
	keywords = {Analysis of Variance, Lasso, Least angle regression, Non-negative garrotte, Piecewise linear solution path},
	file = {Yuan and Lin - 2006 - Model selection and estimation in regression with .pdf:C\:\\Users\\danke\\Zotero\\storage\\6EH4SI59\\Yuan and Lin - 2006 - Model selection and estimation in regression with .pdf:application/pdf}
}

@inproceedings{jacob_group_2009,
	location = {New York, {NY}, {USA}},
	title = {Group Lasso with Overlap and Graph Lasso},
	isbn = {978-1-60558-516-1},
	url = {http://doi.acm.org/10.1145/1553374.1553431},
	doi = {10.1145/1553374.1553431},
	series = {{ICML} '09},
	abstract = {We propose a new penalty function which, when used as regularization for empirical risk minimization procedures, leads to sparse estimators. The support of the sparse vector is typically a union of potentially overlapping groups of co-variates defined a priori, or a set of covariates which tend to be connected to each other when a graph of covariates is given. We study theoretical properties of the estimator, and illustrate its behavior on simulated and breast cancer gene expression data.},
	pages = {433--440},
	booktitle = {ICML2009},	
	publisher = {{ACM}},
	author = {Jacob, Laurent and Obozinski, Guillaume and Vert, Jean-Philippe},
	urldate = {2018-10-08},
	date = {2009},
	file = {ACM Full Text PDF:C\:\\Users\\danke\\Zotero\\storage\\NJU4SBUJ\\Jacob et al. - 2009 - Group Lasso with Overlap and Graph Lasso.pdf:application/pdf}
}